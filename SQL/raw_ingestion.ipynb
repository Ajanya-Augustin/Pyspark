{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68994635",
   "metadata": {},
   "source": [
    "## Ingest raw files to table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eed4d1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "# conf = SparkConf().setMaster('local')\n",
    "# sc = SparkContext(conf = conf)\n",
    "# spark = SparkSession(sc)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"view_creation\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .getOrCreate()\n",
    "# spark = SparkSession.builder \\ \n",
    "#     .master('loacl') \\\n",
    "#Sets the Spark master URL to connect to, “local” to run locally\n",
    "#     .appName(\"view_creation\") \\ \n",
    "#Sets a name for the application, which will be shown in the Spark web UI.\n",
    "#     .config(\"spark.sql.catalogImplementation\", \"hive\") \\ \n",
    "#config(SparkConf conf) :Sets a list of config options based on the given SparkConf.\n",
    "#config(String key, boolean value) :Sets a config option.\n",
    "#     .getOrCreate()\n",
    "#Gets an existing SparkSession or, if there is no existing one, creates a new one based on the options set in this builder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "675df275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('CREATE DATABASE IF NOT EXISTS f1_raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da91d1f7",
   "metadata": {},
   "source": [
    "### csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a02cb38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.sql('DROP TABLE f1_raw.circuits')\n",
    "spark.sql('CREATE TABLE IF NOT EXISTS f1_raw.circuits(circuitId INT,circuitRef STRING,name STRING,location STRING,country STRING,lat DOUBLE,lng DOUBLE,alt INT,url STRING) USING csv OPTIONS (path \"file:/E:/unused/Udemy/Spark_practice/raw/raw_files/circuits.csv\", header true)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed9ab76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------------------+------------+---------+--------+-------+---+--------------------+\n",
      "|circuitId| circuitRef|                name|    location|  country|     lat|    lng|alt|                 url|\n",
      "+---------+-----------+--------------------+------------+---------+--------+-------+---+--------------------+\n",
      "|        1|albert_park|Albert Park Grand...|   Melbourne|Australia|-37.8497|144.968| 10|http://en.wikiped...|\n",
      "|        2|     sepang|Sepang Internatio...|Kuala Lumpur| Malaysia| 2.76083|101.738| 18|http://en.wikiped...|\n",
      "+---------+-----------+--------------------+------------+---------+--------+-------+---+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM f1_raw.circuits;').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2e038d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.sql('DROP TABLE f1_raw.races')\n",
    "spark.sql('CREATE TABLE IF NOT EXISTS f1_raw.races(raceId INT, year INT, round INT, circuitId INT, name STRING, date DATE, time STRING, url STRING) USING csv OPTIONS (path \"file:/E:/unused/Udemy/Spark_practice/raw/raw_files/races.csv\", header true)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0131ffd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+---------+--------------------+----------+--------+--------------------+\n",
      "|raceId|year|round|circuitId|                name|      date|    time|                 url|\n",
      "+------+----+-----+---------+--------------------+----------+--------+--------------------+\n",
      "|     1|2009|    1|        1|Australian Grand ...|2009-03-29|06:00:00|http://en.wikiped...|\n",
      "|     2|2009|    2|        2|Malaysian Grand Prix|2009-04-05|09:00:00|http://en.wikiped...|\n",
      "+------+----+-----+---------+--------------------+----------+--------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM f1_raw.races;').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26ea37b",
   "metadata": {},
   "source": [
    "### single line json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d891676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.sql('DROP TABLE IF EXISTS f1_raw.constructors')\n",
    "spark.sql('CREATE TABLE IF NOT EXISTS f1_raw.constructors( constructorId INT, constructorRef STRING, name STRING, nationality STRING, url STRING) USING json OPTIONS(path \"file:/E:/unused/Udemy/Spark_practice/raw/raw_files/constructors.json\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1678954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+----------+-----------+--------------------+\n",
      "|constructorId|constructorRef|      name|nationality|                 url|\n",
      "+-------------+--------------+----------+-----------+--------------------+\n",
      "|            1|       mclaren|   McLaren|    British|http://en.wikiped...|\n",
      "|            2|    bmw_sauber|BMW Sauber|     German|http://en.wikiped...|\n",
      "+-------------+--------------+----------+-----------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM f1_raw.constructors').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8558f1",
   "metadata": {},
   "source": [
    "### complex json (nested object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ece25fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.sql('DROP TABLE IF EXISTS f1_raw.drivers')\n",
    "spark.sql('CREATE TABLE IF NOT EXISTS f1_raw.drivers( driverId INT, driverRef STRING, number INT, code STRING, name STRUCT<forename: STRING, surname: STRING>, dob DATE, nationality STRING, url STRING) USING json OPTIONS (path \"file:/E:/unused/Udemy/Spark_practice/raw/raw_files/drivers.json\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02788bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------+----+-----------------+----------+-----------+--------------------+\n",
      "|driverId|driverRef|number|code|             name|       dob|nationality|                 url|\n",
      "+--------+---------+------+----+-----------------+----------+-----------+--------------------+\n",
      "|       1| hamilton|    44| HAM|{Lewis, Hamilton}|1985-01-07|    British|http://en.wikiped...|\n",
      "|       2| heidfeld|  null| HEI| {Nick, Heidfeld}|1977-05-10|     German|http://en.wikiped...|\n",
      "+--------+---------+------+----+-----------------+----------+-----------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM f1_raw.drivers').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ec1588",
   "metadata": {},
   "source": [
    "### single line json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcbe9171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.sql('DROP TABLE IF EXISTS f1_raw.results')\n",
    "spark.sql('CREATE TABLE IF NOT EXISTS f1_raw.results( resultId INT, raceId INT, driverId INT, constructorId INT, number INT,grid INT, position INT, positionText STRING, positionOrder INT, points INT, laps INT, time STRING, milliseconds INT, fastestLap INT, rank INT, fastestLapTime STRING, fastestLapSpeed FLOAT, statusId STRING) USING json OPTIONS(path \"file:/E:/unused/Udemy/Spark_practice/raw/raw_files/results.json\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f9d3c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------+-------------+------+----+--------+------------+-------------+------+----+-----------+------------+----------+----+--------------+---------------+--------+\n",
      "|resultId|raceId|driverId|constructorId|number|grid|position|positionText|positionOrder|points|laps|       time|milliseconds|fastestLap|rank|fastestLapTime|fastestLapSpeed|statusId|\n",
      "+--------+------+--------+-------------+------+----+--------+------------+-------------+------+----+-----------+------------+----------+----+--------------+---------------+--------+\n",
      "|       1|    18|       1|            1|    22|   1|       1|           1|            1|    10|  58|1:34:50.616|     5690616|        39|   2|      1:27.452|          218.3|       1|\n",
      "|       2|    18|       2|            2|     3|   5|       2|           2|            2|     8|  58|     +5.478|     5696094|        41|   3|      1:27.739|        217.586|       1|\n",
      "+--------+------+--------+-------------+------+----+--------+------------+-------------+------+----+-----------+------------+----------+----+--------------+---------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM f1_raw.results').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a704db6c",
   "metadata": {},
   "source": [
    "### multiline JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d1a155a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.sql('DROP TABLE IF EXISTS f1_raw.pit_stops')\n",
    "spark.sql('CREATE TABLE IF NOT EXISTS f1_raw.pit_stops( driverId INT, duration STRING, lap INT, milliseconds INT, raceId INT, stop INT, time STRING) USING json OPTIONS(path \"file:/E:/unused/Udemy/Spark_practice/raw/raw_files/pit_stops.json\", multiLine true) ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7e5c9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---+------------+------+----+--------+\n",
      "|driverId|duration|lap|milliseconds|raceId|stop|    time|\n",
      "+--------+--------+---+------------+------+----+--------+\n",
      "|     153|  26.898|  1|       26898|   841|   1|17:05:23|\n",
      "|      30|  25.021|  1|       25021|   841|   1|17:05:52|\n",
      "+--------+--------+---+------------+------+----+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM f1_raw.pit_stops').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef3c8cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.sql('DROP TABLE IF EXISTS f1_raw.lap_times;')\n",
    "spark.sql('CREATE TABLE IF NOT EXISTS f1_raw.lap_times( raceId INT, driverId INT, lap INT, position INT, time STRING, milliseconds INT ) USING csv OPTIONS (path \"file:/E:/unused/Udemy/Spark_practice/raw/raw_files/lap_times\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccf7f03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+--------+--------+------------+\n",
      "|raceId|driverId|lap|position|    time|milliseconds|\n",
      "+------+--------+---+--------+--------+------------+\n",
      "|   841|      20|  1|       1|1:38.109|       98109|\n",
      "|   841|      20|  2|       1|1:33.006|       93006|\n",
      "+------+--------+---+--------+--------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM f1_raw.lap_times').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d78c6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.sql('DROP TABLE IF EXISTS f1_raw.qualifying;')\n",
    "spark.sql('CREATE TABLE IF NOT EXISTS f1_raw.qualifying( constructorId INT, driverId INT, number INT, position INT, q1 STRING, q2 STRING, q3 STRING, qualifyId INT, raceId INT) USING json OPTIONS (path \"file:/E:/unused/Udemy/Spark_practice/raw/raw_files/qualifying\", multiLine true)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14596fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+------+--------+--------+--------+--------+---------+------+\n",
      "|constructorId|driverId|number|position|      q1|      q2|      q3|qualifyId|raceId|\n",
      "+-------------+--------+------+--------+--------+--------+--------+---------+------+\n",
      "|            1|       1|    22|       1|1:26.572|1:25.187|1:26.714|        1|    18|\n",
      "|            2|       9|     4|       2|1:26.103|1:25.315|1:26.869|        2|    18|\n",
      "+-------------+--------+------+--------+--------+--------+--------+---------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM f1_raw.qualifying').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b6b84c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                    |comment|\n",
      "+----------------------------+-------------------------------------------------------------+-------+\n",
      "|constructorId               |int                                                          |null   |\n",
      "|driverId                    |int                                                          |null   |\n",
      "|number                      |int                                                          |null   |\n",
      "|position                    |int                                                          |null   |\n",
      "|q1                          |string                                                       |null   |\n",
      "|q2                          |string                                                       |null   |\n",
      "|q3                          |string                                                       |null   |\n",
      "|qualifyId                   |int                                                          |null   |\n",
      "|raceId                      |int                                                          |null   |\n",
      "|                            |                                                             |       |\n",
      "|# Detailed Table Information|                                                             |       |\n",
      "|Database                    |f1_raw                                                       |       |\n",
      "|Table                       |qualifying                                                   |       |\n",
      "|Owner                       |user                                                         |       |\n",
      "|Created Time                |Mon Jan 15 20:05:58 IST 2024                                 |       |\n",
      "|Last Access                 |UNKNOWN                                                      |       |\n",
      "|Created By                  |Spark 3.3.4                                                  |       |\n",
      "|Type                        |EXTERNAL                                                     |       |\n",
      "|Provider                    |json                                                         |       |\n",
      "|Location                    |file:/E:/unused/Udemy/Spark_practice/raw/raw_files/qualifying|       |\n",
      "+----------------------------+-------------------------------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE EXTENDED f1_raw.qualifying\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d06b8dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-----------+\n",
      "|namespace|tableName   |isTemporary|\n",
      "+---------+------------+-----------+\n",
      "|f1_raw   |circuits    |false      |\n",
      "|f1_raw   |constructors|false      |\n",
      "|f1_raw   |drivers     |false      |\n",
      "|f1_raw   |lap_times   |false      |\n",
      "|f1_raw   |pit_stops   |false      |\n",
      "|f1_raw   |qualifying  |false      |\n",
      "|f1_raw   |races       |false      |\n",
      "|f1_raw   |results     |false      |\n",
      "+---------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW TABLES IN f1_raw').show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7befa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
