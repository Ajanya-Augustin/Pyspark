{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecc11020",
   "metadata": {},
   "source": [
    "## INCREMENTAL LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a465e539",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_date = '2021-04-18'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e892f388",
   "metadata": {},
   "source": [
    "<div style=\"max-width:1400px;margin-center: auto\">\n",
    "<img src=\"images\\pitstop.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0233e977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from delta import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "#  Create a spark session with Delta\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"dbcreation\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "# Create spark context\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85045644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "420403a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pit_stops_schema = StructType(fields=[StructField(\"raceId\", IntegerType(), False),\n",
    "                                      StructField(\"driverId\", IntegerType(), True),\n",
    "                                      StructField(\"stop\", StringType(), True),\n",
    "                                      StructField(\"lap\", IntegerType(), True),\n",
    "                                      StructField(\"time\", StringType(), True),\n",
    "                                      StructField(\"duration\", StringType(), True),\n",
    "                                      StructField(\"milliseconds\", IntegerType(), True)\n",
    "                                     ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0730a80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+---+--------+--------+------------+\n",
      "|raceId|driverId|stop|lap|    time|duration|milliseconds|\n",
      "+------+--------+----+---+--------+--------+------------+\n",
      "|  1053|     839|   1|  1|15:05:16|  30.866|       30866|\n",
      "|  1053|      20|   1|  3|15:10:09|  32.024|       32024|\n",
      "+------+--------+----+---+--------+--------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pit_stops_df = spark.read.json(f'raw files\\\\{file_date}\\\\pit_stops.json', \n",
    "                             schema = pit_stops_schema, \n",
    "                            multiLine = True)\n",
    "pit_stops_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4846d151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp,count,desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97b1e421",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pit_stops_df.withColumnRenamed(\"driverId\", \"driver_id\") \\\n",
    ".withColumnRenamed(\"raceId\", \"race_id\") \\\n",
    ".withColumn(\"ingestion_date\", current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48b623f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"common_functions.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a213262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_condition = \"tgt.race_id = src.race_id AND tgt.driver_id = src.driver_id AND tgt.stop = src.stop AND tgt.race_id = src.race_id\"\n",
    "merge_delta_data(final_df, 'default', 'pit_stops', 'E:/unused/Udemy/Spark_practice/raw/Delta%20lake/spark-warehouse',merge_condition,'race_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b6ac693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_file1_count for each raceId : \n",
      "+-------+--------+\n",
      "|race_id|count(1)|\n",
      "+-------+--------+\n",
      "|   1053|      56|\n",
      "|   1052|      40|\n",
      "|   1047|      23|\n",
      "|   1046|      39|\n",
      "|   1045|      57|\n",
      "|   1044|      38|\n",
      "|   1043|      30|\n",
      "|   1042|      25|\n",
      "|   1041|      33|\n",
      "|   1040|      24|\n",
      "|   1039|      66|\n",
      "|   1038|      37|\n",
      "|   1037|      20|\n",
      "|   1036|      35|\n",
      "|   1035|      41|\n",
      "|   1034|      22|\n",
      "|   1033|      45|\n",
      "|   1032|      21|\n",
      "|   1031|      38|\n",
      "|   1030|      25|\n",
      "+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql('SELECT * FROM default.pit_stops')\n",
    "print('df_file1_count for each raceId : ')\n",
    "df.groupBy('race_id').agg(count('*')).orderBy(desc('race_id')).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
