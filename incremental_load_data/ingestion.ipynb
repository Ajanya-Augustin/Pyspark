{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fcc5b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local') \\\n",
    "    .appName(\"delta\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c00a2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_processed = spark.sql('CREATE DATABASE IF NOT EXISTS f1_processed LOCATION \"file:/E:/unused/Udemy/Spark_practice/raw/incremental_load_data/f1_processed\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49396c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_trans = spark.sql('CREATE DATABASE IF NOT EXISTS f1_trans LOCATION \"file:/E:/unused/Udemy/Spark_practice/raw/incremental_load_data/f1_trans\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a713e55",
   "metadata": {},
   "source": [
    "circuits, races, constructors, drivers - full load scenario. Overwrite data with the new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062812ce",
   "metadata": {},
   "source": [
    "Results, Pitstops, LapTimes and Qualifying files which contain only the data related to that race in the file being received for that week.  \n",
    "Not overwrite but append the data to the Data Lake and in case of failures and we rerun a file, we would want to replace only that files data rather than the entire Data Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5105b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.sql('DROP TABLE f1_processed.qualifying')\n",
    "# spark.sql('DROP TABLE f1_processed.pit_stops')\n",
    "spark.sql('DROP TABLE f1_processed.lap_times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5df34086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----------+\n",
      "|   namespace|   tableName|isTemporary|\n",
      "+------------+------------+-----------+\n",
      "|f1_processed|    circuits|      false|\n",
      "|f1_processed|constructors|      false|\n",
      "|f1_processed|     drivers|      false|\n",
      "|f1_processed|       races|      false|\n",
      "|f1_processed|     results|      false|\n",
      "+------------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW TABLES IN f1_processed').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf83179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
