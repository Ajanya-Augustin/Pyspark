{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adc2f24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_date = '2021-04-18'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43276131",
   "metadata": {},
   "source": [
    "<div style=\"max-width:1400px;margin-center: auto\">\n",
    "<img src=\"images\\race join.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a220f665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local') \\\n",
    "    .appName(\"race_join\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c327ec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "races_df = spark.read.csv(f'raw files\\\\{file_date}\\\\races.csv', header =True, inferSchema =True).\\\n",
    "            withColumnRenamed('year','race_year').\\\n",
    "            withColumnRenamed('name','race_name').\\\n",
    "            withColumnRenamed('date','race_date')\n",
    "constructors_df = spark.read.parquet(f'f1_processed\\\\constructors').\\\n",
    "            withColumnRenamed('name','team')\n",
    "drivers_df = spark.read.parquet(f'f1_processed\\\\drivers').\\\n",
    "            withColumnRenamed('name','driver_name').\\\n",
    "            withColumnRenamed('number','driver_number').\\\n",
    "            withColumnRenamed('nationality','driver_nationality')\n",
    "circuit_df = spark.read.parquet(f'f1_processed\\\\circuits').\\\n",
    "                    withColumnRenamed('location','circuit_location')\n",
    "results_df = spark.read.parquet(f'f1_processed\\\\results') \\\n",
    "            .filter(f\"file_date = '{file_date}'\") \\\n",
    "            .withColumnRenamed(\"time\", \"race_time\") \\\n",
    "            .withColumnRenamed(\"race_id\", \"result_race_id\") \\\n",
    "            .withColumnRenamed(\"file_date\", \"result_file_date\") \n",
    "#filter used to process only the data for the current date\n",
    "races_df.show(2)\n",
    "constructors_df.show(2)\n",
    "drivers_df.show(2)\n",
    "circuit_df.show(2)\n",
    "results_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fb96947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56b31fb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fnl_df = results_df.join(constructors_df, results_df.constructor_id == constructors_df.constructor_id,'inner').\\\n",
    "        join(drivers_df,results_df.driver_id == drivers_df.driver_id,'inner').\\\n",
    "        join(races_df,results_df.result_race_id == races_df.raceId,'inner').\\\n",
    "        join(circuit_df,races_df.circuitId == circuit_df.circuit_id,'inner').\\\n",
    "        select(races_df.race_year,\n",
    "               races_df.raceId,\n",
    "               races_df.race_name,\n",
    "               races_df.race_date,\n",
    "               circuit_df.circuit_location,\n",
    "               drivers_df.driver_name,\n",
    "               drivers_df.driver_number,\n",
    "               drivers_df.driver_nationality,\n",
    "               constructors_df.team,\n",
    "               results_df.grid,\n",
    "               results_df.fastest_lap,\n",
    "               results_df.race_time,\n",
    "               results_df.points,\n",
    "              results_df.position).withColumn(\"created_date\", current_timestamp()) \\\n",
    "                          .withColumnRenamed(\"result_file_date\", \"file_date\")\\\n",
    "                          .withColumnRenamed(\"raceId\", \"race_id\")\n",
    "fnl_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4407bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"common_functions.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9506fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite_partition(fnl_df, 'f1_trans', 'race_results', 'race_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f38cadbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|race_id|count(1)|\n",
      "+-------+--------+\n",
      "|   1053|      20|\n",
      "|   1052|      20|\n",
      "|   1047|      20|\n",
      "|   1046|      20|\n",
      "|   1045|      20|\n",
      "|   1044|      20|\n",
      "|   1043|      20|\n",
      "|   1042|      20|\n",
      "|   1041|      20|\n",
      "|   1040|      20|\n",
      "|   1039|      20|\n",
      "|   1038|      20|\n",
      "|   1037|      20|\n",
      "|   1036|      20|\n",
      "|   1035|      20|\n",
      "|   1034|      20|\n",
      "|   1033|      20|\n",
      "|   1032|      20|\n",
      "|   1031|      20|\n",
      "|   1030|      20|\n",
      "+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df= spark.sql('SELECT race_id,count(*) FROM f1_trans.race_results GROUP BY race_id ORDER BY race_id DESC')\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
